// Run edge probing models over CoVe,
// without training an encoder on pre-training tasks.
//
// Use this for baseline probing & hyperparameter tuning for probing models.

// This imports the defaults, which can be overridden below.
include "defaults.conf"  // relative path to this file

run_prefix = pretrainers_ // gets prefixed for everything in train_openai
openai_transformer = 1
openai_transformer_fine_tune = 1
openai_transformer_fine_tune_min_layer = -1
weighted_openai_transformer = 0
weighted_openai_transformer_only_fine_tune_weights = 0
load_model = 0


exp_name = repretrain
run_name = mrpc
 
classifier = log_reg
pair_attn = 0

max_seq_len = 100


train_tasks = "cola"
eval_tasks = "cola"

// Eval will use task-specific params.
do_train = 1        // skip main train phase
allow_untrained_encoder_parameters = 1  // allow skipping training phase
allow_missing_task_map = 1  // ignore missing classifier_task_map.json
train_for_eval = 0  // train using eval task params
do_eval = 1
// load_eval_checkpoint = "/scratch/tjf324/jiant/repretrain/pretrainers_mnli/model_state_main_epoch_21.best_macro.th"
//load_eval_checkpoint = "/scratch/tjf324/jiant/repretrain/fake"
write_preds = none
dropout = 0.1 // Before the classifier, same as in OpenAI







task_patience = 2  // vals until LR decay
patience = 8      // vals until early-stopping

batch_size = 8

cove = 0
word_embs = "none"
elmo = 0

// Use no-op encoder (no params).
sent_enc = "null"
skip_embs = 1  // forward embeddings from lower level.
sep_embs_for_skip = 1  // use task embeddings since we skip the generic ones.

// Optimizer
optimizer = openai_adam
max_grad_norm = 1
lr = 0.0000625
num_epoch_openai_finetune = 3

project_pooler = 0

# Classifiers
#cola
cola_d_proj = 768
cola_classifier_dropout = 0.1 // This should not matter as long as we use log_reg

#mrpc
mrpc_double_sim_d_proj = 768
mrpc_double_sim_classifier_dropout = 0.1 // This should not matter as long as we use log_reg 

#sst
sst_d_proj = 768
sst_classifier_dropout = 0.1

#qqp
qqp_double_sim_d_proj = 768
qqp_double_sim_classifier_dropout = 0.1 // This should not matter as long as we use log_reg

#stsb
stsb_double_sim_d_proj = 768
stsb_double_sim_classifier_dropout = 0.1 // This should not matter as long as we use log_reg

#wnli
wnli_single_seq_d_proj = 768
wnli_single_seq_classifier_dropout = 0.1 // This should not matter as long as we use log_reg 


#qnli
qnli_single_seq_d_proj = 768
qnli_single_seq_classifier_dropout = 0.1 // This should not matter as long as qe use log_reg

#mnli
mnli_single_seq_d_proj = 768
mnli_single_seq_classifier_dropout = 0.1 // This should not matter as long as qe use log_reg 

#rte
rte_single_seq_d_proj = 768
rte_single_seq_classifier_dropout = 0.1 // This should not matter as long as we use log_reg 

#fake_sentence_detection
fake_sentence_detection_d_proj = 768
fake_sentence_detection_classifier_dropout = 0.1 // This should not matter as long as we use log_reg

#reddit_sarcasm
reddit_sarcasm_d_proj = 768
reddit_sarcasm_classifier_dropout = 0.1 // This should not matter as long as we use log_reg

// Avoid decaying as we already have a LR scheduler inside the optimizer
// task_patience = 9999999999999
// lr_decay_factor = 0.9999999999
