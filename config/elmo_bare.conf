// Elmo without an LSTM on top
include "defaults.conf"
exp_name = elmo_finetune // Van share the name since tokenization is the same
run_name = test

train_tasks = cola
eval_tasks = cola


do_train = 1
train_for_eval = 0
do_eval = 1

load_model = 0

word_embs = "none"  // The type of word embedding layer. Usually set to none when using ELMo.
                  // Options: none, scratch (i.e., trained from scratch), glove, fastText.
char_filter_sizes = "2,3,4,5"  // Size of char CNN filters.
elmo = 1  // If true, load and use ELMo.
elmo_chars_only = 0  // If true, use *only* the char CNN layer of ELMo. If false but elmo is true, use the full ELMo.

skip_embs = 1


sent_enc = "null"
elmo_finetune_all = 1 // Whether to fine-tune the whole ELMO model on train_for_eval


sep_embs_for_skip = 1  // use task embeddings since we skip the generic ones.
// This name is very outdated. Required for both OpenAI and bare ELMo


